### 주식 가격 분석 모델 개발 일지

<br/>

### 2023.7.24 - 2023.8.3

#### 진행사항
  - 주가 데이터 수집과 전처리 및 metadata 형식으로 정리 진행
  - 주가 분석 모델 툴킷 제작 및 모델 설계 (CNN + RNN 기반) 진행
  - 하이퍼 파라미터 설정 및 학습 진행

<br/>

### 2023.8.6

#### 현재 문제점
  - 현재, 주가 분석 모델 툴킷을 제작하여 훈련을 진행하고 있는 중이다.
  - 70 epochs 를 최종 목표로 하고 있는 도중, 10, 20, 30 epochs 일 때의 Training 과 Validation 에 대한 MSE 와 Loss 를 보면서 모델 품질에 대하여 판단하는 중이다.
  - 그 결과, Training 에 대해서는 mse 가 대폭 줄어들고 있지만, Validation 에 대해서는 mse 가 줄고 있지 않는 상태를 발견하였다.

#### 예상되는 원인
  - 현재 metadata.txt 에서 시장과 종목 코드를 기준으로 정렬하게 해두었는데, 이로 인하여 Validation 쪽에 변동폭이 큰 종목들이 대거 들어가게 되어 그렇게 된 것 같다.

#### 데이터에 대한 고찰 (23.8.6)

- 가격 움직임에 대하여 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율이 비슷한가?
  - 변동성에 대한 지표를 바탕으로 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율을 조사한다. 이것도 히스토그램으로 얼마나 분포되어 있는지 확인할 수 있어야 함.
  
- 수익률이 기존 가격에 근접한 종목과 큰 수익을 달성하는 종목의 비율이 어떻게 구성되어 있나?
  - 히스토그램을 바탕으로 수익에 있어서 10% 단위에 대하여 데이터의 분산을 확인한다. 
  - 만약, 비율이 다르다면 어떻게 진행하는 것이 좋을 것 같나?
  - 일단, train 과 validation 데이터를 골고루 분산시키는 것이 좋겠다. metadata.txt 에 있는 데이터들을 기반으로 각 데이터들에 대하여 sort 를 진행하고 만약 train 과 validation 사이의 관계가 10:1 이라면 (전체데이터개수)/11 (=x) 을 계산하여 10x 를 train 데이터에 넣고, x 를 validation 데이터에 집어넣는 방식으로 진행.
  - 비교적 변동성이 큰 종목의 데이터 개수가 모자랄 것이라는 생각이 든다. 이에 대하여 데이터 개수가 부족한 부분에 대하여 데이터 증강을 통하여 접근할 수 있도록하면 좋겠다는 생각을 하였다.
  
- 데이터 증강의 경우, 정확하게 어떤 방식을 선택할 것인가?
  - 현재, Time Domain 에 있어서 기본적으로 제안하는 증강 기법은 Window cropping or slicing, Window warping, Flipping, Noise injection, Label expansion 정도가 있다. 나에게 적합한 방식은 Noise Injection 가 될 것 같다. 노이즈로는 gaussian noise, spike, step-like trend, and slope-like trend 정도가 있다고 한다. 최종적으로 gaussian noise 를 적용할 예정이다.

<br/>

### 2023.8.10

#### EDA 접근
![23 8 10 데이터 불균형 해소를 위한 수식 적용](https://github.com/DevTae/StockDatabasePreview/assets/55177359/593d553d-d40e-4168-bb26-716d4cf692a5)
- 결과층 데이터가 특정 구간에 몰려 있는 것을 확인 / 이러한 부분이 데이터 불균형을 뜻하고 학습 평가 검증이 다소 어려워질 수 있음
- 따라서, 결과값 확대 및 축소 함수(특정 공식)를 적용하여 다음과 같이 **정규 분포 형태의 데이터 분포**로 구성할 수 있었다.
- 이전의 문제점을 해결할 수 있어 당장에는 Data Augmentation 을 적용하지 않고 학습을 진행하고자 한다.
- 데이터 수는 약 160만 개로 충분히 커버가 가능할 듯 보인다.
